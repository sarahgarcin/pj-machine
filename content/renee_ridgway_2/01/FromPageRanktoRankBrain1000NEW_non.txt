# From Page Rank <br> to RankBrain
## Renée Ridgway
  
  
The concept of Page Rank has its basis in the Scientific Citation Index (SCI), a form of academic hierarchy that has now been grafted as a conceptual paradigm for the way we find information and how that information is prioritised for us. The eponymous Page Rank algorithm was developed in 1998 and is basically a popularity contest based on votes. A link coming from a node with a high rank has more value than a link coming from a node with low rank. The scheme therefore assigns two scores for each page: its authority, which estimates the value of the content of the page, and its hub value, which estimates the value of its links to other pages.  
  
PR(A) = (1-d) + d (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))  
  
### Secret recipes
Presently, “keyword search” is still the way Google Search organises the internet by crawling and indexing,[1] which determines the importance of a website based on the words it contains, how often other sites link to it, and dozens of other measures. With Google Search the emphasis is to keep the attention of the user and to have them click on the higher rankings, effortlessly. However as Gillespie points out, the exact workings are opaque and vary for diverse users, “the criteria and code of algorithms are generally obscured—but not equally or from everyone” (Gillespie 185). Based on users’ histories, location and search terms, the searcher is “personalised” through a set of criteria.[2] Not only are the creators of content of web pages kept in check by search engines, but the tracking of different factors, or signals, determine the ranking of an individual page. Mostly through reverse engineering, a whole “Search Engine Optimisation” (SEO) industry has developed around “gaming” the algorithm to figure out its recipe or signals.  
  
### Signals
During the past 18 years, Google has constantly tweaked their proprietary algorithm, containing around 200 ingredients or “signals” in the recipe.[3] “Signals are typically factors that are tied to content, such as the words on a page, the links pointing at a page, whether a page is on a secure server and so on. They can also be tied to a user, such as where a searcher is located or their search and browsing history.”[4] Links, content, keyword density, words in bold, duplicate content, domain registration duration and outbound link quality are some other examples of factors, or “clues”. One of the major changes in 2010 to the core algorithm of Page Rank was the “Caffeine” update, which enabled an improvement in the gathering of information or indexing, instead of just sorting. “Panda” was an update that was implemented in 2011 that downranks sites, which are considered lower quality, enabling higher quality pages to rise. In April 2012 Google launched the “Penguin” update that attempts to catch sites, and now devalues spam instead of demoting (adjusting the rank) of the entire site. As of September 30, 2016, it updates in real time as part of the core algorithm.[5]   
  
Analogous to the components of engine that has had it parts replaced, where Penguin and Panda might be the oil filter and gas pump respectively, the launch of “Hummingbird” in August 2013 was Google’s largest overhaul since 2001. With the introduction of a brand new engine the emphasis has shifted to the contextual — it’s less now about the keyword and more about the intention behind it — the semantic capabilities are what are at stake. Whereas previously certain keywords were the focus, at the moment  the other words in the sentence and their meaning are accentuated. Within this field of “semantic search” the “relationality linking search queries and web documents”[6] is reflected with the “Knowledge Graph,”[7] along with “conversational search” that incorporates voice activated enquiries.    
