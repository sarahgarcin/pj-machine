path: content/geoff_cox/02a

----

content: 

----

index: 0

----

zoom: 1

----

xPos: 9.157894736842156

----

yPos: 1.9999999999999998

----

wordSpace: 0

----

nbOfFiles: 1

----

text: There is a sense in which the world begins to be reproduced through computational models and algorithmic logic, changing what and how we see, think and even behave. Subjects are produced in relation to what algorithms understand about our intentions, gestures, behaviours, opinions, or desires, through aggregating massive amounts of data (data mining) and machine learning (the predictive practices of data mining).[8] That machines learn is accounted for through a combination of calculative practices that help to approximate what will likely happen through the use of different algorithms and models. The difficulty lies in to what extent these generalisations are accurate, or to what degree the predictive model is valid, or “able to generalise” sufficiently well. Hence the “learners” (machine learning algorithms), although working at the level of generalisation, are also highly contextual and specific to the fields in which they operate in a coming together of what Adrian Mackenzie calls a “play of truth and falsehood”.[9]  
   
Thus what constitutes knowledge can be seen to be controlled and arranged in new ways that invoke Berger’s earlier call for skepticism. Antoinette Rouvroy is similarly concerned that algorithms begin to define what counts for knowledge as a further case of subjectivation, as we are unable to substantively intervene in these processes of how knowledge is produced.[10] Her claim is that knowledge is delivered “without truth” through the increasing use of machines that filter it through the use of search engines that have no interest in content as such or detail on how knowledge is generated. Instead they privilege real-time relational infrastructures that subsume the knowledge of workers and machines into generalised assemblages as techniques of “algorithmic governmentality”.[11]  
   
In this sense, the knowledge produced is bound together with systems of power that are more and more visual and hence ambiguous in character. And clearly computers further complicate the field of visuality, and ways of seeing, especially in relation to the interplay of knowledge and power. Aside from the totalizing aspects (that I have outlined thus far), there are also significant “points of slippage or instability” of epistemic authority,[12] or what Berger would no doubt identify as the further unsettling of the relations between seeing and knowing. So, if algorithms can be understood as seeing, in what sense, and under what conditions? Algorithms are ideological only inasmuch as they are part of larger infrastructures and assemblages.   
   
But to ask whether machines can see or not is the wrong question to ask, rather we should discuss how machines have changed the nature of seeing and hence our knowledge of the world.[13] In this we should not try to oppose machine and human seeing but take them to be more thoroughly entangled — a more “posthuman” or “new materialist” position that challenges the onto-epistemological character of seeing — and produces new kinds of knowledge-power that both challenges as well as extends the anthropomorphism of vision and its attachment to dominant forms of rationality. Clearly there are other (nonhuman) perspectives that also illuminate our understanding of the world. This pedagogic (and political) impulse is perfectly in keeping with Ways of Seeing and its project of visual literacy.[14] What is required is an expansion of this ethic to algorithmic literacy to examine how machine vision unsettles the relations between what we see and what we know in new ways. 


----

blockSize: 8

----

