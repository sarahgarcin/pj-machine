# Ways of Machine Seeing<span class=small-text-execption>[1]</span>
## Geoff Cox

You are looking at the front cover of the book _Ways of Seeing_ written by John Berger in 1972. The text is the script of the TV series, and if you’ve seen the programmes, you can almost hear the distinctive pedagogic tone of Berger’s voice as you read his words: “The relation between what we see and what we know is never settled.”[2]  

The image by Magritte on the cover further emphasises the point about the deep ambiguity of images and the always-present difficulty of legibility between words and seeing.[3] In addition to the explicit reference to the “artwork” essay by Walter Benjamin,[4] the TV programme employed Brechtian techniques, such as revealing the technical apparatus of the studio; to encourage viewers not to simply watch (or read) in an easy way but rather to be forced into an analysis of elements of “separation” that would lead to a “return from alienation”.[5] Berger further reminded the viewer of the specifics of the technical reproduction in use and its ideological force in a similar manner:  
“But remember that I am controlling and using for my own purposes the means of reproduction needed for these programmes [...] with this programme as with all programmes, you receive images and meanings which are arranged. I hope you will consider what I arrange but please remain skeptical of it.”  
   
That you are not really looking at the book as such but a scanned image of a book — viewable by means of an embedded link to a server where the image is stored — testifies to the ways in which what, and how, we see and know is further unsettled through complex assemblages of elements. The increasing use of relational machines such as search engines is a good example of the ways in which knowledge is filtered at the expense of the more specific detail on how it was produced. Knowledge is now produced in relation to planetary computational infrastructures in which other agents such as algorithms generalise massive amounts of (big) data.[6]   
  
Clearly algorithms do not act alone or with magical (totalising) power but exist as part of larger infrastructures and ideologies. Some well-publicised recent cases have come to public attention that exemplify a contemporary politics (and crisis) of representation in this way, such as the Google search results for “three black teenagers” and “three white teenagers” (mug shots and happy teens at play, respectively).[7] The problem is one of learning in its widest sense, and “machine learning” techniques are employed on data to produce forms of knowledge that are inextricably bound to hegemonic systems of power and prejudice.   
   
 


